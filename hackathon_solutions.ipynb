{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Linker\n",
    "\n",
    "The goal here is to produce a map of IDs to datasets\n",
    "\n",
    "I'll be using the two Bloomberg Estimate Americas datasets\n",
    "\n",
    "#### Useful info\n",
    "\n",
    "* Primary keys for BEst Americas - idBbGlobal,idBbUnique,bestFperiodOverride\n",
    "\n",
    "* Their old mnemonic forms are ID_BB_GLOBAL, ID_BB_UNIQUE, BEST_FPERIOD_OVERRIDE\n",
    "\n",
    "* The first three columns of a given record in an out file correspond to: record identifier, return code (meaningful in .dif files), number of columns (every record should have the same number)\n",
    "\n",
    "The .OUT files are organized by holding the column information like this:\n",
    "\n",
    "START-OF-FIELDS\n",
    "\n",
    "column names\n",
    "\n",
    "END-OF-FIELDS\n",
    "\n",
    "and the data like this:\n",
    "\n",
    "START-OF-DATA\n",
    "\n",
    "data\n",
    "\n",
    "END-OF-DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General problem setup\n",
    "\n",
    "I want to\n",
    "    * open a file, read its contents\n",
    "    * separate the header info and the data\n",
    "    * find my keys in the header\n",
    "    * roll through the data, grabbing each record with a valid primary key\n",
    "    * repeat for each dataset we're opening\n",
    "    * create a map from primary key to dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I broke this out into two different functions since the first one was getting kinda long!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collect_keys(datasets, primary_key):\n",
    "    \"\"\" return all the primary keys in some datasets\n",
    "        Arguments:\n",
    "            datasets (list): a list of paths to datasets\n",
    "            primary_key (list): a list of fields, in their old mnemonic form\n",
    "        Returns:\n",
    "            a list of lists, where each \n",
    "    \"\"\"\n",
    "    all_keys = []\n",
    "\n",
    "    for dataset in datasets:\n",
    "        dataset_keys = []\n",
    "        # open the file, read in the data\n",
    "        file = open(dataset)\n",
    "        file_content = file.readlines()\n",
    "        clean_file_content = []\n",
    "        for line in file_content:\n",
    "            line = line.strip() # removes the trailing \\n from each line\n",
    "            clean_file_content.append(line)\n",
    "\n",
    "        # header parsing\n",
    "        start_of_header = clean_file_content.index('START-OF-FIELDS')\n",
    "        end_of_header = clean_file_content.index('END-OF-FIELDS')\n",
    "        header = clean_file_content[start_of_header+1:end_of_header]\n",
    "\n",
    "        # data parsing\n",
    "        start_of_data = clean_file_content.index('START-OF-DATA')\n",
    "        end_of_data = clean_file_content.index('END-OF-DATA')\n",
    "        data = clean_file_content[start_of_data+1:end_of_data]\n",
    "\n",
    "        # primary key indices\n",
    "        key_indices = []\n",
    "        for key in primary_key:\n",
    "            key_indices.append(header.index(key))\n",
    "        \n",
    "        # walk through data, collecting primary keys\n",
    "        for row in data:\n",
    "            pkey = []\n",
    "            row_data = row.split('|')\n",
    "            for key_index in key_indices:\n",
    "                pkey.append(row_data[key_index + 3]) # offset of 3 since those cols aren't helpful here\n",
    "            pkey_str = '_'.join(pkey)# produces this: BBG000BCSCB1_EQ0010017100001000_2005Q4\n",
    "            dataset_keys.append(pkey_str)\n",
    "\n",
    "        # we've collected every record in the file, time to add it to our output\n",
    "        all_keys.append(dataset_keys)\n",
    "\n",
    "    return all_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_keys(datasets, all_keys):\n",
    "    \"\"\" create a map of keys and datasets\n",
    "        Arguments:\n",
    "            datasets (list): a list of datasets\n",
    "            all_keys (list): a list of lists primary keys found in each dataset\n",
    "        Returns:\n",
    "            a dictionary mapping the keys to datasets\n",
    "    \"\"\"\n",
    "    # we can work this way because we know that we have the same number of datasets as key lists\n",
    "    key_dict = {}\n",
    "    for i in range(len(datasets)):\n",
    "        for key in all_keys[i]:\n",
    "            if key in key_dict:\n",
    "                key_dict[key].append(datasets[i])\n",
    "            else:\n",
    "                key_dict[key] = [datasets[i]]\n",
    "    return key_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "primary_keys = ['ID_BB_GLOBAL', 'ID_BB_UNIQUE', 'BEST_FPERIOD_OVERRIDE']\n",
    "datasets = ['data/20191212_best_amer.out', 'data/20191204_best_amer.out']\n",
    "key_lists = collect_keys(datasets, primary_keys)\n",
    "key_dict = map_keys(datasets, key_lists)\n",
    "\n",
    "with open('example_key_mapping.csv', 'w') as output_file:\n",
    "    # remember, dictionary.items() gives us the key, value pairs in our for loop!\n",
    "    for key, file_list in key_dict.items():\n",
    "        file_str = ','.join(file_list)\n",
    "        output_file.write(key + ',' + file_str + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .OUT file parser\n",
    "\n",
    "Shares a bit of similarities with the first function for file linking\n",
    "\n",
    "\n",
    "#### General problem setup\n",
    "    * Open the file, read in the data\n",
    "    * separate the header info and the data\n",
    "    * load into pandas\n",
    "    * save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def out_to_dataframe(dataset):\n",
    "    \"\"\" convert an out file into a pandas DataFrame\n",
    "        Arguments:\n",
    "            dataset (str): path to the OUT file\n",
    "        Returns:\n",
    "            a pandas DataFrame\n",
    "    \"\"\"\n",
    "    file = open(dataset)\n",
    "    file_content = file.readlines()\n",
    "    clean_file_content = []\n",
    "    for line in file_content:\n",
    "        line = line.strip() # removes the trailing \\n from each line\n",
    "        clean_file_content.append(line)\n",
    "\n",
    "    # header parsing\n",
    "    start_of_header = clean_file_content.index('START-OF-FIELDS')\n",
    "    end_of_header = clean_file_content.index('END-OF-FIELDS')\n",
    "    header = clean_file_content[start_of_header+1:end_of_header]\n",
    "\n",
    "    # data parsing\n",
    "    start_of_data = clean_file_content.index('START-OF-DATA')\n",
    "    end_of_data = clean_file_content.index('END-OF-DATA')\n",
    "    data = clean_file_content[start_of_data+1:end_of_data]\n",
    "    \n",
    "    clean_data = []\n",
    "    for row in data:\n",
    "        row_data = row.split('|')\n",
    "        # ignore first three columns, final | has an empty value at the end\n",
    "        row_data = row_data[3:-1]\n",
    "        clean_data.append(row_data)\n",
    "\n",
    "    dataframe = pd.DataFrame(data=clean_data, columns=header)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = out_to_dataframe('data/20191204_best_amer.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
